{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cda2182",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17458da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Parquet Data...\n",
      ">>> Resampling to 15min bars...\n",
      ">>> Creating Delta Target...\n",
      "Data Prepared (tabular). Rows: 35130\n",
      "NN Sequences: X=(35115, 16, 7), y=(35115,)\n",
      "NN Split: train=25283, val=2809, test=7023\n",
      "\n",
      ">>> Tuning RNN Hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2026-01-14 20:19:49,729] Trial 15 failed with parameters: {'rnn_type': 'LSTM', 'hidden_size': 76, 'num_layers': 2, 'dropout': 0.23338622430617414, 'lr': 0.0015294488790853964, 'weight_decay': 2.8842752469673902e-06, 'batch_size': 128} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shah/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/rz/hb914cgn7wb_sxdhbbm8krnh0000gn/T/ipykernel_71391/1963971102.py\", line 311, in objective_rnn\n",
      "    scores.append(train_with_early_stopping(model, train_loader, val_loader, lr=lr, weight_decay=wd))\n",
      "                  ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/rz/hb914cgn7wb_sxdhbbm8krnh0000gn/T/ipykernel_71391/1963971102.py\", line 222, in train_with_early_stopping\n",
      "    yhat = model(xb)\n",
      "  File \"/Users/shah/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shah/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/rz/hb914cgn7wb_sxdhbbm8krnh0000gn/T/ipykernel_71391/1963971102.py\", line 259, in forward\n",
      "    out, _ = self.rnn(x)\n",
      "             ~~~~~~~~^^^\n",
      "  File \"/Users/shah/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shah/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/shah/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/rnn.py\", line 1127, in forward\n",
      "    result = _VF.lstm(\n",
      "        input,\n",
      "    ...<7 lines>...\n",
      "        self.batch_first,\n",
      "    )\n",
      "KeyboardInterrupt\n",
      "[W 2026-01-14 20:19:49,800] Trial 15 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 317\u001b[39m\n\u001b[32m    315\u001b[39m optuna.logging.set_verbosity(optuna.logging.WARNING)\n\u001b[32m    316\u001b[39m study_rnn = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m \u001b[43mstudy_rnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest RNN Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy_rnn.best_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    320\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> Tuning CNN Hyperparameters...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/optuna/study/_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 311\u001b[39m, in \u001b[36mobjective_rnn\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    308\u001b[39m     val_loader   = DataLoader(SeqDataset(X_va_s, y_va), batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    310\u001b[39m     model = RNNRegressor(n_features, hidden_size, num_layers, dropout, rnn_type=rnn_type).to(DEVICE)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     scores.append(\u001b[43mtrain_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwd\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np.mean(scores))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mtrain_with_early_stopping\u001b[39m\u001b[34m(model, train_loader, val_loader, lr, weight_decay, max_epochs, patience)\u001b[39m\n\u001b[32m    220\u001b[39m xb = xb.to(DEVICE)\n\u001b[32m    221\u001b[39m yb = yb.to(DEVICE)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m yhat = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m vpred.append(yhat.detach().cpu().numpy().reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m    224\u001b[39m vtrue.append(yb.detach().cpu().numpy().reshape(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 259\u001b[39m, in \u001b[36mRNNRegressor.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.head(out[:, -\u001b[32m1\u001b[39m, :])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CODE_BOOK_3/MLII/MLFinalProject/.venv/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1127\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1124\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1139\u001b[39m     result = _VF.lstm(\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1141\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1148\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1149\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ==========================================\n",
    "# 1) CONFIGURATION (NO TEST LEAKAGE)\n",
    "# ==========================================\n",
    "TIME_WINDOW = \"15min\"\n",
    "FILE_PATH   = \"../data/derivative_data_2025.parquet\"\n",
    "N_TRIALS    = 20\n",
    "\n",
    "SEQ_LEN     = 16\n",
    "MAX_EPOCHS  = 60\n",
    "PATIENCE    = 10\n",
    "RANDOM_SEED = 42\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "VAL_FRAC = 0.10  # validation slice from the end of the training block\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# ==========================================\n",
    "# 2) DATA LOADING (SAME LOGIC)\n",
    "# ==========================================\n",
    "print(\">>> Loading Parquet Data...\")\n",
    "df = pd.read_parquet(FILE_PATH, engine=\"pyarrow\")\n",
    "\n",
    "if pd.api.types.is_numeric_dtype(df[\"timestamp\"]):\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "else:\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "df = df[df[\"instrument_name\"].str.contains(\"BTC\")]\n",
    "\n",
    "split_names = df[\"instrument_name\"].str.split(\"-\", expand=True)\n",
    "df[\"Expiry_Str\"] = split_names[1]\n",
    "df[\"Strike\"]     = split_names[2].astype(float)\n",
    "df[\"Type\"]       = split_names[3]\n",
    "df[\"Expiry\"]     = pd.to_datetime(df[\"Expiry_Str\"], format=\"%d%b%y\", errors=\"coerce\")\n",
    "\n",
    "df[\"days_to_expiry\"] = (df[\"Expiry\"] - df[\"timestamp\"]).dt.total_seconds() / (24 * 3600)\n",
    "df[\"moneyness\"]      = df[\"index_price\"] / df[\"Strike\"]\n",
    "df[\"iv\"]             = pd.to_numeric(df[\"iv\"], errors=\"coerce\")\n",
    "df = df[df[\"iv\"] > 0]\n",
    "\n",
    "# ==========================================\n",
    "# 3) FEATURE ENGINEERING (SAME LOGIC)\n",
    "# ==========================================\n",
    "print(f\">>> Resampling to {TIME_WINDOW} bars...\")\n",
    "\n",
    "def calculate_features(sub_df: pd.DataFrame):\n",
    "    if sub_df.empty:\n",
    "        return None\n",
    "    stats = {}\n",
    "\n",
    "    atm_mask = sub_df[\"moneyness\"].between(0.98, 1.02)\n",
    "    if atm_mask.any():\n",
    "        stats[\"ATM_IV\"] = sub_df.loc[atm_mask, \"iv\"].mean()\n",
    "    else:\n",
    "        stats[\"ATM_IV\"] = sub_df[\"iv\"].mean()\n",
    "\n",
    "    put_iv  = sub_df.loc[sub_df[\"Type\"] == \"P\", \"iv\"].mean()\n",
    "    call_iv = sub_df.loc[sub_df[\"Type\"] == \"C\", \"iv\"].mean()\n",
    "    put_iv  = put_iv  if pd.notna(put_iv)  else stats[\"ATM_IV\"]\n",
    "    call_iv = call_iv if pd.notna(call_iv) else stats[\"ATM_IV\"]\n",
    "    stats[\"Skew\"] = put_iv - call_iv\n",
    "\n",
    "    near_mask = sub_df[\"days_to_expiry\"] <= 10\n",
    "    far_mask  = sub_df[\"days_to_expiry\"] > 10\n",
    "    iv_near = sub_df.loc[near_mask, \"iv\"].mean()\n",
    "    iv_far  = sub_df.loc[far_mask,  \"iv\"].mean()\n",
    "    iv_near = iv_near if (pd.notna(iv_near) and iv_near > 0) else stats[\"ATM_IV\"]\n",
    "    iv_far  = iv_far  if pd.notna(iv_far) else stats[\"ATM_IV\"]\n",
    "    stats[\"Term_Structure\"] = iv_far / iv_near\n",
    "\n",
    "    stats[\"Close_Price\"] = sub_df[\"index_price\"].iloc[-1]\n",
    "    stats[\"Volume\"]      = sub_df[\"amount\"].sum()\n",
    "\n",
    "    return pd.Series(stats)\n",
    "\n",
    "df_grouped = df.set_index(\"timestamp\").resample(TIME_WINDOW).apply(calculate_features)\n",
    "df_grouped = df_grouped.ffill()\n",
    "\n",
    "# ==========================================\n",
    "# 4) TARGET CREATION (SAME LOGIC)\n",
    "# ==========================================\n",
    "print(\">>> Creating Delta Target...\")\n",
    "\n",
    "df_grouped[\"ATM_IV_Change\"]      = df_grouped[\"ATM_IV\"].diff()\n",
    "df_grouped[\"ATM_IV_Change_Lag1\"] = df_grouped[\"ATM_IV_Change\"].shift(1)\n",
    "\n",
    "df_grouped[\"returns\"]      = np.log(df_grouped[\"Close_Price\"] / df_grouped[\"Close_Price\"].shift(1))\n",
    "df_grouped[\"Realized_Vol\"] = df_grouped[\"returns\"].shift(1).rolling(window=4).std()\n",
    "\n",
    "df_grouped[\"Target_Delta\"] = df_grouped[\"ATM_IV\"].shift(-1) - df_grouped[\"ATM_IV\"]\n",
    "\n",
    "model_data = df_grouped.dropna()\n",
    "\n",
    "feature_cols = [\"ATM_IV\", \"Skew\", \"Term_Structure\", \"Realized_Vol\",\n",
    "                \"Volume\", \"ATM_IV_Change\", \"ATM_IV_Change_Lag1\"]\n",
    "\n",
    "X = model_data[feature_cols]\n",
    "y = model_data[\"Target_Delta\"]\n",
    "\n",
    "print(f\"Data Prepared (tabular). Rows: {len(model_data)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5) SEQUENCE BUILD (SAME LOGIC)\n",
    "# ==========================================\n",
    "def make_sequences(X_df: pd.DataFrame, y_s: pd.Series, seq_len: int):\n",
    "    Xv = X_df.values.astype(np.float32)\n",
    "    yv = y_s.values.astype(np.float32)\n",
    "\n",
    "    X_seq, y_seq = [], []\n",
    "    for t in range(seq_len - 1, len(Xv)):\n",
    "        X_seq.append(Xv[t - seq_len + 1 : t + 1, :])\n",
    "        y_seq.append(yv[t])\n",
    "    return np.stack(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = make_sequences(X, y, SEQ_LEN)\n",
    "n_samples, _, n_features = X_seq.shape\n",
    "print(f\"NN Sequences: X={X_seq.shape}, y={y_seq.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5.5) SPLIT (FIXED: TRAIN/VAL/TEST CHRONOLOGICAL)\n",
    "# ==========================================\n",
    "split_idx = int(n_samples * 0.8)  # test starts here\n",
    "\n",
    "X_train_full, y_train_full = X_seq[:split_idx], y_seq[:split_idx]\n",
    "X_test,       y_test       = X_seq[split_idx:], y_seq[split_idx:]\n",
    "\n",
    "val_size  = max(1, int(len(X_train_full) * VAL_FRAC))\n",
    "val_start = len(X_train_full) - val_size\n",
    "\n",
    "X_train, y_train = X_train_full[:val_start], y_train_full[:val_start]\n",
    "X_val,   y_val   = X_train_full[val_start:], y_train_full[val_start:]\n",
    "\n",
    "print(f\"NN Split: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6) TORCH HELPERS\n",
    "# ==========================================\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_seq, y_seq):\n",
    "        self.X = torch.from_numpy(X_seq)\n",
    "        self.y = torch.from_numpy(y_seq).unsqueeze(-1)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def fit_scaler_on_train(X_train_seq):\n",
    "    scaler = StandardScaler()\n",
    "    N, T, F = X_train_seq.shape\n",
    "    scaler.fit(X_train_seq.reshape(N * T, F))\n",
    "    return scaler\n",
    "\n",
    "def apply_scaler(scaler, X_seq):\n",
    "    N, T, F = X_seq.shape\n",
    "    return scaler.transform(X_seq.reshape(N * T, F)).reshape(N, T, F).astype(np.float32)\n",
    "\n",
    "def rmse_np(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_model(model, X_seq_np, batch_size=1024):\n",
    "    model.eval()\n",
    "    ds = SeqDataset(X_seq_np, np.zeros((len(X_seq_np),), dtype=np.float32))\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    preds = []\n",
    "    for xb, _ in dl:\n",
    "        xb = xb.to(DEVICE)\n",
    "        preds.append(model(xb).detach().cpu().numpy().reshape(-1))\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, val_loader, lr, weight_decay=0.0,\n",
    "                              max_epochs=MAX_EPOCHS, patience=PATIENCE):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_rmse = np.inf\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for _epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            yhat = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # val RMSE\n",
    "        model.eval()\n",
    "        vpred, vtrue = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "                yhat = model(xb)\n",
    "                vpred.append(yhat.detach().cpu().numpy().reshape(-1))\n",
    "                vtrue.append(yb.detach().cpu().numpy().reshape(-1))\n",
    "        vpred = np.concatenate(vpred)\n",
    "        vtrue = np.concatenate(vtrue)\n",
    "        val_rmse = rmse_np(vtrue, vpred)\n",
    "\n",
    "        if val_rmse < best_rmse - 1e-8:\n",
    "            best_rmse = val_rmse\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return best_rmse\n",
    "\n",
    "# ==========================================\n",
    "# 7) MODELS\n",
    "# ==========================================\n",
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size, num_layers, dropout, rnn_type=\"GRU\"):\n",
    "        super().__init__()\n",
    "        rnn_cls = nn.GRU if rnn_type.upper() == \"GRU\" else nn.LSTM\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=(dropout if num_layers > 1 else 0.0),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Dropout(dropout), nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.head(out[:, -1, :])\n",
    "\n",
    "class CNN1DRegressor(nn.Module):\n",
    "    def __init__(self, n_features, channels, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(n_features, channels, kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channels, channels, kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Flatten(), nn.Dropout(dropout), nn.Linear(channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        return self.head(self.net(x))\n",
    "\n",
    "# ==========================================\n",
    "# 8) OPTUNA TUNING (NO TEST LEAKAGE)\n",
    "#   - CV only uses X_train_full (train+val block)\n",
    "# ==========================================\n",
    "print(\"\\n>>> Tuning RNN Hyperparameters...\")\n",
    "\n",
    "def objective_rnn(trial):\n",
    "    seed_everything(RANDOM_SEED)\n",
    "\n",
    "    rnn_type    = trial.suggest_categorical(\"rnn_type\", [\"GRU\", \"LSTM\"])\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128)\n",
    "    num_layers  = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout     = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "    lr          = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    wd          = trial.suggest_float(\"weight_decay\", 1e-8, 1e-2, log=True)\n",
    "    batch_size  = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    scores = []\n",
    "\n",
    "    for tr_idx, va_idx in tscv.split(X_train_full):\n",
    "        X_tr, y_tr = X_train_full[tr_idx], y_train_full[tr_idx]\n",
    "        X_va, y_va = X_train_full[va_idx], y_train_full[va_idx]\n",
    "\n",
    "        scaler = fit_scaler_on_train(X_tr)\n",
    "        X_tr_s = apply_scaler(scaler, X_tr)\n",
    "        X_va_s = apply_scaler(scaler, X_va)\n",
    "\n",
    "        train_loader = DataLoader(SeqDataset(X_tr_s, y_tr), batch_size=batch_size, shuffle=False)\n",
    "        val_loader   = DataLoader(SeqDataset(X_va_s, y_va), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = RNNRegressor(n_features, hidden_size, num_layers, dropout, rnn_type=rnn_type).to(DEVICE)\n",
    "        scores.append(train_with_early_stopping(model, train_loader, val_loader, lr=lr, weight_decay=wd))\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study_rnn = optuna.create_study(direction=\"minimize\")\n",
    "study_rnn.optimize(objective_rnn, n_trials=N_TRIALS)\n",
    "print(f\"Best RNN Params: {study_rnn.best_params}\")\n",
    "\n",
    "print(\"\\n>>> Tuning CNN Hyperparameters...\")\n",
    "\n",
    "def objective_cnn(trial):\n",
    "    seed_everything(RANDOM_SEED)\n",
    "\n",
    "    channels    = trial.suggest_int(\"channels\", 16, 128)\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5, 7])\n",
    "    dropout     = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "    lr          = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    wd          = trial.suggest_float(\"weight_decay\", 1e-8, 1e-2, log=True)\n",
    "    batch_size  = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    scores = []\n",
    "\n",
    "    for tr_idx, va_idx in tscv.split(X_train_full):\n",
    "        X_tr, y_tr = X_train_full[tr_idx], y_train_full[tr_idx]\n",
    "        X_va, y_va = X_train_full[va_idx], y_train_full[va_idx]\n",
    "\n",
    "        scaler = fit_scaler_on_train(X_tr)\n",
    "        X_tr_s = apply_scaler(scaler, X_tr)\n",
    "        X_va_s = apply_scaler(scaler, X_va)\n",
    "\n",
    "        train_loader = DataLoader(SeqDataset(X_tr_s, y_tr), batch_size=batch_size, shuffle=False)\n",
    "        val_loader   = DataLoader(SeqDataset(X_va_s, y_va), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = CNN1DRegressor(n_features, channels, kernel_size, dropout).to(DEVICE)\n",
    "        scores.append(train_with_early_stopping(model, train_loader, val_loader, lr=lr, weight_decay=wd))\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "study_cnn = optuna.create_study(direction=\"minimize\")\n",
    "study_cnn.optimize(objective_cnn, n_trials=N_TRIALS)\n",
    "print(f\"Best CNN Params: {study_cnn.best_params}\")\n",
    "\n",
    "# ==========================================\n",
    "# 9) FINAL TRAINING & EVALUATION (FIXED)\n",
    "#   - Fit scaler on TRAIN only\n",
    "#   - Early stop on VAL only\n",
    "#   - Test untouched until final metrics\n",
    "# ==========================================\n",
    "def evaluate_final(model_name, build_model, best_params):\n",
    "    print(f\"\\n>>> Training Final {model_name}...\")\n",
    "\n",
    "    batch_size = int(best_params.get(\"batch_size\", 128))\n",
    "    lr = float(best_params[\"lr\"])\n",
    "    wd = float(best_params[\"weight_decay\"])\n",
    "\n",
    "    # scaler on TRAIN only\n",
    "    scaler = fit_scaler_on_train(X_train)\n",
    "    X_train_s = apply_scaler(scaler, X_train)\n",
    "    X_val_s   = apply_scaler(scaler, X_val)\n",
    "    X_test_s  = apply_scaler(scaler, X_test)\n",
    "\n",
    "    train_loader = DataLoader(SeqDataset(X_train_s, y_train), batch_size=batch_size, shuffle=False)\n",
    "    val_loader   = DataLoader(SeqDataset(X_val_s,   y_val),   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = build_model().to(DEVICE)\n",
    "    _ = train_with_early_stopping(model, train_loader, val_loader, lr=lr, weight_decay=wd)\n",
    "\n",
    "    # test untouched until here\n",
    "    y_pred = predict_model(model, X_test_s)\n",
    "\n",
    "    final_rmse = rmse_np(y_test, y_pred)\n",
    "    naive_rmse = rmse_np(y_test, np.zeros_like(y_test))\n",
    "\n",
    "    pred_signs   = np.sign(y_pred)\n",
    "    actual_signs = np.sign(y_test)\n",
    "    valid_idx = actual_signs != 0\n",
    "    dir_acc = accuracy_score(actual_signs[valid_idx], pred_signs[valid_idx])\n",
    "\n",
    "    print(f\"\\n=== {model_name} RESULTS ===\")\n",
    "    print(f\"Model RMSE:   {final_rmse:.6f}\")\n",
    "    print(f\"Baseline RMSE:{naive_rmse:.6f}\")\n",
    "    print(f\"Improvement:  {((naive_rmse - final_rmse)/naive_rmse)*100:.2f}%\")\n",
    "    print(f\"Directional Accuracy: {dir_acc*100:.2f}%\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test[:100], label=\"Actual Change\", alpha=0.7)\n",
    "    plt.plot(y_pred[:100], label=f\"{model_name} Pred\", linestyle=\"--\")\n",
    "    plt.axhline(0, lw=0.5)\n",
    "    plt.title(f\"{model_name}: ATM IV Change Prediction (Delta)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return final_rmse, dir_acc\n",
    "\n",
    "# ---- Final RNN ----\n",
    "bestRNN = study_rnn.best_params\n",
    "\n",
    "def make_final_rnn():\n",
    "    return RNNRegressor(\n",
    "        n_features=n_features,\n",
    "        hidden_size=int(bestRNN[\"hidden_size\"]),\n",
    "        num_layers=int(bestRNN[\"num_layers\"]),\n",
    "        dropout=float(bestRNN[\"dropout\"]),\n",
    "        rnn_type=str(bestRNN[\"rnn_type\"])\n",
    "    )\n",
    "\n",
    "rmse_rnn, dir_rnn = evaluate_final(\"RNN\", make_final_rnn, bestRNN)\n",
    "\n",
    "# ---- Final CNN ----\n",
    "bestCNN = study_cnn.best_params\n",
    "\n",
    "def make_final_cnn():\n",
    "    return CNN1DRegressor(\n",
    "        n_features=n_features,\n",
    "        channels=int(bestCNN[\"channels\"]),\n",
    "        kernel_size=int(bestCNN[\"kernel_size\"]),\n",
    "        dropout=float(bestCNN[\"dropout\"])\n",
    "    )\n",
    "\n",
    "rmse_cnn, dir_cnn = evaluate_final(\"CNN\", make_final_cnn, bestCNN)\n",
    "\n",
    "print(\"\\n>>> Summary\")\n",
    "print(f\"RNN  RMSE={rmse_rnn:.6f} | DirAcc={dir_rnn*100:.2f}%\")\n",
    "print(f\"CNN  RMSE={rmse_cnn:.6f} | DirAcc={dir_cnn*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06559dbb",
   "metadata": {},
   "source": [
    "## CNN RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0869c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ==========================================\n",
    "# 0) CONFIG\n",
    "# ==========================================\n",
    "TIME_WINDOW = \"15min\"\n",
    "FILE_PATH   = \"../data/derivative_data_2025.parquet\"\n",
    "N_TRIALS    = 20\n",
    "\n",
    "SEQ_LEN     = 16\n",
    "MAX_EPOCHS  = 60\n",
    "PATIENCE    = 10\n",
    "RANDOM_SEED = 42\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VAL_FRAC    = 0.10  # val slice from end of training block\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# ==========================================\n",
    "# 1) DATA LOADING\n",
    "# ==========================================\n",
    "print(\">>> Loading Parquet Data...\")\n",
    "df = pd.read_parquet(FILE_PATH, engine=\"pyarrow\")\n",
    "\n",
    "if pd.api.types.is_numeric_dtype(df[\"timestamp\"]):\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "else:\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "df = df[df[\"instrument_name\"].str.contains(\"BTC\")]\n",
    "\n",
    "split_names = df[\"instrument_name\"].str.split(\"-\", expand=True)\n",
    "df[\"Expiry_Str\"] = split_names[1]\n",
    "df[\"Strike\"]     = split_names[2].astype(float)\n",
    "df[\"Type\"]       = split_names[3]\n",
    "df[\"Expiry\"]     = pd.to_datetime(df[\"Expiry_Str\"], format=\"%d%b%y\", errors=\"coerce\")\n",
    "\n",
    "df[\"days_to_expiry\"] = (df[\"Expiry\"] - df[\"timestamp\"]).dt.total_seconds() / (24 * 3600)\n",
    "df[\"moneyness\"]      = df[\"index_price\"] / df[\"Strike\"]\n",
    "df[\"iv\"]             = pd.to_numeric(df[\"iv\"], errors=\"coerce\")\n",
    "df = df[df[\"iv\"] > 0]\n",
    "\n",
    "# ==========================================\n",
    "# 2) FEATURE ENGINEERING (15-min bars)\n",
    "# ==========================================\n",
    "print(f\">>> Resampling to {TIME_WINDOW} bars...\")\n",
    "\n",
    "def calculate_features(sub_df: pd.DataFrame):\n",
    "    if sub_df.empty:\n",
    "        return None\n",
    "    stats = {}\n",
    "\n",
    "    atm_mask = sub_df[\"moneyness\"].between(0.98, 1.02)\n",
    "    stats[\"ATM_IV\"] = sub_df.loc[atm_mask, \"iv\"].mean() if atm_mask.any() else sub_df[\"iv\"].mean()\n",
    "\n",
    "    put_iv  = sub_df.loc[sub_df[\"Type\"] == \"P\", \"iv\"].mean()\n",
    "    call_iv = sub_df.loc[sub_df[\"Type\"] == \"C\", \"iv\"].mean()\n",
    "    put_iv  = put_iv  if pd.notna(put_iv)  else stats[\"ATM_IV\"]\n",
    "    call_iv = call_iv if pd.notna(call_iv) else stats[\"ATM_IV\"]\n",
    "    stats[\"Skew\"] = put_iv - call_iv\n",
    "\n",
    "    near_mask = sub_df[\"days_to_expiry\"] <= 10\n",
    "    far_mask  = sub_df[\"days_to_expiry\"] > 10\n",
    "    iv_near = sub_df.loc[near_mask, \"iv\"].mean()\n",
    "    iv_far  = sub_df.loc[far_mask,  \"iv\"].mean()\n",
    "    iv_near = iv_near if (pd.notna(iv_near) and iv_near > 0) else stats[\"ATM_IV\"]\n",
    "    iv_far  = iv_far  if pd.notna(iv_far) else stats[\"ATM_IV\"]\n",
    "    stats[\"Term_Structure\"] = iv_far / iv_near\n",
    "\n",
    "    stats[\"Close_Price\"] = sub_df[\"index_price\"].iloc[-1]\n",
    "    stats[\"Volume\"]      = sub_df[\"amount\"].sum()\n",
    "    return pd.Series(stats)\n",
    "\n",
    "df_grouped = df.set_index(\"timestamp\").resample(TIME_WINDOW).apply(calculate_features)\n",
    "df_grouped = df_grouped.ffill()\n",
    "\n",
    "# --- Option B: time-of-day + day-of-week cyclical encodings ---\n",
    "ts = df_grouped.index\n",
    "tod = ts.hour + ts.minute / 60.0\n",
    "df_grouped[\"tod_sin\"] = np.sin(2 * np.pi * tod / 24.0)\n",
    "df_grouped[\"tod_cos\"] = np.cos(2 * np.pi * tod / 24.0)\n",
    "\n",
    "dow = ts.dayofweek.astype(float)  # 0..6\n",
    "df_grouped[\"dow_sin\"] = np.sin(2 * np.pi * dow / 7.0)\n",
    "df_grouped[\"dow_cos\"] = np.cos(2 * np.pi * dow / 7.0)\n",
    "\n",
    "# ==========================================\n",
    "# 3) TARGET CREATION (same as LGBM/CatBoost)\n",
    "# ==========================================\n",
    "print(\">>> Creating Delta Target...\")\n",
    "\n",
    "df_grouped[\"ATM_IV_Change\"]      = df_grouped[\"ATM_IV\"].diff()\n",
    "df_grouped[\"ATM_IV_Change_Lag1\"] = df_grouped[\"ATM_IV_Change\"].shift(1)\n",
    "\n",
    "df_grouped[\"returns\"]      = np.log(df_grouped[\"Close_Price\"] / df_grouped[\"Close_Price\"].shift(1))\n",
    "df_grouped[\"Realized_Vol\"] = df_grouped[\"returns\"].shift(1).rolling(window=4).std()\n",
    "\n",
    "df_grouped[\"Target_Delta\"] = df_grouped[\"ATM_IV\"].shift(-1) - df_grouped[\"ATM_IV\"]\n",
    "\n",
    "model_data = df_grouped.dropna()\n",
    "\n",
    "feature_cols = [\n",
    "    \"ATM_IV\",\"Skew\",\"Term_Structure\",\"Realized_Vol\",\"Volume\",\"ATM_IV_Change\",\"ATM_IV_Change_Lag1\",\n",
    "    \"tod_sin\",\"tod_cos\",\"dow_sin\",\"dow_cos\"\n",
    "]\n",
    "\n",
    "X = model_data[feature_cols]\n",
    "y = model_data[\"Target_Delta\"]\n",
    "\n",
    "print(f\"Data Prepared (tabular). Rows: {len(model_data)} | Features: {len(feature_cols)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4) SEQUENCES\n",
    "# ==========================================\n",
    "def make_sequences(X_df: pd.DataFrame, y_s: pd.Series, seq_len: int):\n",
    "    Xv = X_df.values.astype(np.float32)\n",
    "    yv = y_s.values.astype(np.float32)\n",
    "\n",
    "    X_seq, y_seq = [], []\n",
    "    for t in range(seq_len - 1, len(Xv)):\n",
    "        X_seq.append(Xv[t - seq_len + 1 : t + 1, :])\n",
    "        y_seq.append(yv[t])\n",
    "    return np.stack(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = make_sequences(X, y, SEQ_LEN)\n",
    "n_samples, _, n_features = X_seq.shape\n",
    "print(f\"NN Sequences: X={X_seq.shape}, y={y_seq.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5) SPLIT (train/val/test chronological; no leakage)\n",
    "# ==========================================\n",
    "split_idx = int(n_samples * 0.8)  # test starts here\n",
    "X_train_full, y_train_full = X_seq[:split_idx], y_seq[:split_idx]\n",
    "X_test,       y_test       = X_seq[split_idx:], y_seq[split_idx:]\n",
    "\n",
    "val_size  = max(1, int(len(X_train_full) * VAL_FRAC))\n",
    "val_start = len(X_train_full) - val_size\n",
    "\n",
    "X_train, y_train = X_train_full[:val_start], y_train_full[:val_start]\n",
    "X_val,   y_val   = X_train_full[val_start:], y_train_full[val_start:]\n",
    "\n",
    "print(f\"NN Split: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6) TORCH HELPERS\n",
    "# ==========================================\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_seq, y_seq):\n",
    "        self.X = torch.from_numpy(X_seq)                      # (N,T,F)\n",
    "        self.y = torch.from_numpy(y_seq).unsqueeze(-1)        # (N,1)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "def fit_scaler_on_train(X_train_seq):\n",
    "    scaler = StandardScaler()\n",
    "    N, T, F = X_train_seq.shape\n",
    "    scaler.fit(X_train_seq.reshape(N*T, F))\n",
    "    return scaler\n",
    "\n",
    "def apply_scaler(scaler, X_seq):\n",
    "    N, T, F = X_seq.shape\n",
    "    return scaler.transform(X_seq.reshape(N*T, F)).reshape(N, T, F).astype(np.float32)\n",
    "\n",
    "def rmse_np(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_model(model, X_seq_np, batch_size=1024):\n",
    "    model.eval()\n",
    "    dl = DataLoader(SeqDataset(X_seq_np, np.zeros((len(X_seq_np),), dtype=np.float32)),\n",
    "                    batch_size=batch_size, shuffle=False)\n",
    "    preds = []\n",
    "    for xb, _ in dl:\n",
    "        xb = xb.to(DEVICE)\n",
    "        preds.append(model(xb).detach().cpu().numpy().reshape(-1))\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, val_loader, lr, weight_decay=0.0,\n",
    "                              max_epochs=MAX_EPOCHS, patience=PATIENCE):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_rmse = np.inf\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    # for _epoch in range(1, max_epochs + 1):\n",
    "    for _epoch in tqdm(range(1, max_epochs + 1), desc=\"epochs\", leave=False):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            yhat = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # val RMSE\n",
    "        model.eval()\n",
    "        vpred, vtrue = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                yhat = model(xb)\n",
    "                vpred.append(yhat.detach().cpu().numpy().reshape(-1))\n",
    "                vtrue.append(yb.detach().cpu().numpy().reshape(-1))\n",
    "        vpred = np.concatenate(vpred); vtrue = np.concatenate(vtrue)\n",
    "        val_rmse = rmse_np(vtrue, vpred)\n",
    "\n",
    "        if val_rmse < best_rmse - 1e-8:\n",
    "            best_rmse = val_rmse\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return best_rmse\n",
    "\n",
    "# ==========================================\n",
    "# 7) MODELS\n",
    "# ==========================================\n",
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size, num_layers, dropout, rnn_type=\"GRU\"):\n",
    "        super().__init__()\n",
    "        rnn_cls = nn.GRU if rnn_type.upper() == \"GRU\" else nn.LSTM\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=(dropout if num_layers > 1 else 0.0),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Dropout(dropout), nn.Linear(hidden_size, 1))\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.head(out[:, -1, :])\n",
    "\n",
    "class CNN1DRegressor(nn.Module):\n",
    "    def __init__(self, n_features, channels, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(n_features, channels, kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channels, channels, kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Flatten(), nn.Dropout(dropout), nn.Linear(channels, 1))\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # (N,F,T)\n",
    "        return self.head(self.net(x))\n",
    "\n",
    "# ==========================================\n",
    "# 8) OPTUNA TUNING (CV only on train_full; test untouched)\n",
    "# ==========================================\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"\\n>>> Tuning RNN Hyperparameters...\")\n",
    "\n",
    "def objective_rnn(trial):\n",
    "    seed_everything(RANDOM_SEED)\n",
    "\n",
    "    rnn_type    = trial.suggest_categorical(\"rnn_type\", [\"GRU\", \"LSTM\"])\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128)\n",
    "    num_layers  = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout     = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "    lr          = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    wd          = trial.suggest_float(\"weight_decay\", 1e-8, 1e-2, log=True)\n",
    "    batch_size  = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    scores = []\n",
    "\n",
    "    for tr_idx, va_idx in tscv.split(X_train_full):\n",
    "        X_tr, y_tr = X_train_full[tr_idx], y_train_full[tr_idx]\n",
    "        X_va, y_va = X_train_full[va_idx], y_train_full[va_idx]\n",
    "\n",
    "        scaler = fit_scaler_on_train(X_tr)\n",
    "        X_tr_s = apply_scaler(scaler, X_tr)\n",
    "        X_va_s = apply_scaler(scaler, X_va)\n",
    "\n",
    "        train_loader = DataLoader(SeqDataset(X_tr_s, y_tr), batch_size=batch_size, shuffle=False)\n",
    "        val_loader   = DataLoader(SeqDataset(X_va_s, y_va), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = RNNRegressor(n_features, hidden_size, num_layers, dropout, rnn_type=rnn_type).to(DEVICE)\n",
    "        scores.append(train_with_early_stopping(model, train_loader, val_loader, lr=lr, weight_decay=wd))\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "study_rnn = optuna.create_study(direction=\"minimize\")\n",
    "study_rnn.optimize(objective_rnn, n_trials=N_TRIALS)\n",
    "print(f\"Best RNN Params: {study_rnn.best_params}\")\n",
    "\n",
    "print(\"\\n>>> Tuning CNN Hyperparameters...\")\n",
    "\n",
    "def objective_cnn(trial):\n",
    "    seed_everything(RANDOM_SEED)\n",
    "\n",
    "    channels    = trial.suggest_int(\"channels\", 16, 128)\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5, 7])\n",
    "    dropout     = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "    lr          = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    wd          = trial.suggest_float(\"weight_decay\", 1e-8, 1e-2, log=True)\n",
    "    batch_size  = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    scores = []\n",
    "\n",
    "    for tr_idx, va_idx in tscv.split(X_train_full):\n",
    "        X_tr, y_tr = X_train_full[tr_idx], y_train_full[tr_idx]\n",
    "        X_va, y_va = X_train_full[va_idx], y_train_full[va_idx]\n",
    "\n",
    "        scaler = fit_scaler_on_train(X_tr)\n",
    "        X_tr_s = apply_scaler(scaler, X_tr)\n",
    "        X_va_s = apply_scaler(scaler, X_va)\n",
    "\n",
    "        train_loader = DataLoader(SeqDataset(X_tr_s, y_tr), batch_size=batch_size, shuffle=False)\n",
    "        val_loader   = DataLoader(SeqDataset(X_va_s, y_va), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = CNN1DRegressor(n_features, channels, kernel_size, dropout).to(DEVICE)\n",
    "        scores.append(train_with_early_stopping(model, train_loader, val_loader, lr=lr, weight_decay=wd))\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "study_cnn = optuna.create_study(direction=\"minimize\")\n",
    "study_cnn.optimize(objective_cnn, n_trials=N_TRIALS)\n",
    "print(f\"Best CNN Params: {study_cnn.best_params}\")\n",
    "\n",
    "# ==========================================\n",
    "# 9) PERMUTATION IMPORTANCE (NN analogue of \"feature importance\")\n",
    "# ==========================================\n",
    "def permutation_importance_seq(model, X_test_s, y_test, feature_names, batch_size=1024, seed=RANDOM_SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    base_pred = predict_model(model, X_test_s, batch_size=batch_size)\n",
    "    base_rmse = rmse_np(y_test, base_pred)\n",
    "\n",
    "    importances = []\n",
    "    # for j, name in enumerate(feature_names):\n",
    "    for j, name in tqdm(list(enumerate(feature_names)), desc=\"perm-imp\", leave=False):\n",
    "        Xp = X_test_s.copy()\n",
    "        flat = Xp[:, :, j].reshape(-1)\n",
    "        rng.shuffle(flat)\n",
    "        Xp[:, :, j] = flat.reshape(Xp.shape[0], Xp.shape[1])\n",
    "        pred_p = predict_model(model, Xp, batch_size=batch_size)\n",
    "        rmse_p = rmse_np(y_test, pred_p)\n",
    "        importances.append(rmse_p - base_rmse)  # increase in RMSE\n",
    "\n",
    "    return base_rmse, np.array(importances, dtype=float)\n",
    "\n",
    "def plot_feature_importance(importances, feature_names, title):\n",
    "    idx = np.argsort(importances)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(idx)), importances[idx], align=\"center\")\n",
    "    plt.yticks(range(len(idx)), np.array(feature_names)[idx])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 10) FINAL TRAINING & EVALUATION (no leakage)\n",
    "# ==========================================\n",
    "def evaluate_final(model_name, build_model, best_params):\n",
    "    print(f\"\\n>>> Training Final {model_name}...\")\n",
    "\n",
    "    batch_size = int(best_params.get(\"batch_size\", 128))\n",
    "    lr = float(best_params[\"lr\"])\n",
    "    wd = float(best_params[\"weight_decay\"])\n",
    "\n",
    "    # scaler fit on TRAIN only\n",
    "    scaler = fit_scaler_on_train(X_train)\n",
    "    X_train_s = apply_scaler(scaler, X_train)\n",
    "    X_val_s   = apply_scaler(scaler, X_val)\n",
    "    X_test_s  = apply_scaler(scaler, X_test)\n",
    "\n",
    "    train_loader = DataLoader(SeqDataset(X_train_s, y_train), batch_size=batch_size, shuffle=False)\n",
    "    val_loader   = DataLoader(SeqDataset(X_val_s,   y_val),   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = build_model().to(DEVICE)\n",
    "    _ = train_with_early_stopping(model, train_loader, val_loader, lr=lr, weight_decay=wd)\n",
    "\n",
    "    # test untouched until here\n",
    "    y_pred = predict_model(model, X_test_s, batch_size=batch_size)\n",
    "\n",
    "    final_rmse = rmse_np(y_test, y_pred)\n",
    "    naive_rmse = rmse_np(y_test, np.zeros_like(y_test))\n",
    "\n",
    "    pred_signs   = np.sign(y_pred)\n",
    "    actual_signs = np.sign(y_test)\n",
    "    valid_idx = actual_signs != 0\n",
    "    dir_acc = accuracy_score(actual_signs[valid_idx], pred_signs[valid_idx])\n",
    "\n",
    "    print(f\"\\n=== {model_name} RESULTS ===\")\n",
    "    print(f\"Model RMSE:   {final_rmse:.6f}\")\n",
    "    print(f\"Baseline RMSE:{naive_rmse:.6f}\")\n",
    "    print(f\"Improvement:  {((naive_rmse - final_rmse)/naive_rmse)*100:.2f}%\")\n",
    "    print(f\"Directional Accuracy: {dir_acc*100:.2f}%\")\n",
    "\n",
    "    # same output-style graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test[:100], label=\"Actual Change\", color=\"black\", alpha=0.5)\n",
    "    plt.plot(y_pred[:100], label=f\"{model_name} Pred\", linestyle=\"--\")\n",
    "    plt.axhline(0, lw=0.5)\n",
    "    plt.title(f\"{model_name}: ATM IV Change Prediction (Delta)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # NN \"feature importance\" analogue\n",
    "    _, imps = permutation_importance_seq(model, X_test_s, y_test, feature_cols, batch_size=batch_size)\n",
    "    plot_feature_importance(imps, feature_cols, title=f\"{model_name} Feature Importance (Permutation RMSE)\")\n",
    "\n",
    "    return final_rmse, dir_acc\n",
    "\n",
    "# ---- Final RNN ----\n",
    "bestRNN = study_rnn.best_params\n",
    "def make_final_rnn():\n",
    "    return RNNRegressor(\n",
    "        n_features=n_features,\n",
    "        hidden_size=int(bestRNN[\"hidden_size\"]),\n",
    "        num_layers=int(bestRNN[\"num_layers\"]),\n",
    "        dropout=float(bestRNN[\"dropout\"]),\n",
    "        rnn_type=str(bestRNN[\"rnn_type\"])\n",
    "    )\n",
    "\n",
    "rmse_rnn, dir_rnn = evaluate_final(\"RNN\", make_final_rnn, bestRNN)\n",
    "\n",
    "# ---- Final CNN ----\n",
    "bestCNN = study_cnn.best_params\n",
    "def make_final_cnn():\n",
    "    return CNN1DRegressor(\n",
    "        n_features=n_features,\n",
    "        channels=int(bestCNN[\"channels\"]),\n",
    "        kernel_size=int(bestCNN[\"kernel_size\"]),\n",
    "        dropout=float(bestCNN[\"dropout\"])\n",
    "    )\n",
    "\n",
    "rmse_cnn, dir_cnn = evaluate_final(\"CNN\", make_final_cnn, bestCNN)\n",
    "\n",
    "print(\"\\n>>> Summary\")\n",
    "print(f\"RNN  RMSE={rmse_rnn:.6f} | DirAcc={dir_rnn*100:.2f}%\")\n",
    "print(f\"CNN  RMSE={rmse_cnn:.6f} | DirAcc={dir_cnn*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191df9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
